{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet Lab Solution\n",
    "![LeNet Architecture](lenet.png)\n",
    "Source: Yan LeCun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the MNIST data, which comes pre-loaded with TensorFlow.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image Shape: (32, 32, 3)\n",
      "\n",
      "Training Set:   31367 samples\n",
      "Validation Set: 7842 samples\n",
      "Test Set:       12630 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "training_file = \"./data/train.p\"\n",
    "testing_file = \"./data/test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X, y = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "assert(len(X_train) == len(y_train))\n",
    "assert(len(X_validation) == len(y_validation))\n",
    "assert(len(X_test) == len(y_test))\n",
    "\n",
    "print()\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print()\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data that TensorFlow pre-loads comes as 28x28x1 images.\n",
    "\n",
    "However, the LeNet architecture only accepts 32x32xC images, where C is the number of color channels.\n",
    "\n",
    "In order to reformat the MNIST data into a shape that LeNet will accept, we pad the data with two rows of zeros on the top and bottom, and two columns of zeros on the left and right (28+2+2 = 32).\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data\n",
    "\n",
    "View a sample from the dataset.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAH4AAAB6CAYAAAB5sueeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJztvWusZVtW3/cb87X26zyq6r4augEnYDCB2I7BMRgMMlEg\nlkxiKcIYRwRHUWJwJOIvRigoTdqRrWAFOYnTkj8k7VixHVmKbZzE0AaThBDSRsE2MXRD06SBft7b\n9956nDpn77XWnHPkw5hr7V11q+reOlXV9zpVo7R16uyz3mPOMcf4j/8YS1SVZ/L0iXu7L+CZvD3y\nTPFPqTxT/FMqzxT/lMozxT+l8kzxT6k8U/xTKs8U/5TKM8U/pfJM8U+pPDHFi8ifEpGPi8hWRD4k\nIl/7pM71TB5enojiReSPAv858F7gdwO/CHxQRJ57Eud7Jg8v8iSSNCLyIeAfqur3t98F+ATwX6rq\njzz2Ez6Th5bwuA8oIhH4PcCfm75TVRWRnwK+7h7bXwO+FfgNYPe4r+f/x7IAvgT4oKq+9rA7P3bF\nA88BHnj5ru9fBr78Htt/K/DXnsB1PC3yx4G//rA7PQnFP6z8BkDXRcYx06UICAis10vW6wUKqNoH\nQERwIrzyymu864VrIALA4aKlQNFKrYoDnMIrr17nxReuTIdH2rZVbYeqimptxxFefe0GV6+eUg8O\nrrrfTxBcOxYivPr6dZ67dgVU2jkUAZwDce0aq1JVCSK8/Op1vuQLruGjxwfPmDNjLpSq7X7t2s/P\nt5yd92z7kfUq2b2VysXFMD+/h5UnofhXgQK8eNf3LwKfvcf2O4AXnr/Ka6/f5F3ves4U6xxOBEXt\nmavOChERBME7z2LRYY94ft6AKXEslVwrEQgieOdYdR04McWL6bM2ZaCK1nl04Z2jWyTKwaA79Inc\n9BE7vxMhhjT/XdRGlPOC987O1xTqHTjniF1HSJ4QPW70yDhSq84DTYDVIvLSC46Pf+JVvuLLvwAB\nbp/v+Kcf/uT8/B5WHrtXr6oj8AvAt0zfNefuW4Cfu++FOFNocA7flG4zeZrN009FtX1QalVKrQcf\nU2Jt29iTZtacoDgUO7wNIOfsvMF5YgjEEIkhIc4RQocPEe8Drl2bNAtDsxSqe0VNA1VVqbTr2D8I\nRGyATPdVVClVycV+agWtZoXqdPn7wyOqiCr+EX3yJ2XqfxT4KyLyC8DPA38aWAF/5X47yKT44IH5\nmdr/D8wsyvRoQaHUCggVqGIjWYT5iYkComYOJtO7f4xtlbBB5po1wXlUPE48MXZQK7UWtH2o1ZaH\n2RrZIbQdarrg2TgczF47p85f14opvNiydMc4bZetIu36QSo40XbHl5cnonhV/ZstZn8fZuL/CfCt\nqvq5++3jxCGAd4LO6+3BiJ9n8HyS/ewHKsw/HUzj4vCqgGndPVjh26w3a+MJ3uNcQHwkeM/Ras1Q\nlZxHxjyQ8wg5U7U05dx1loPv5gEqe79E2yiZ7kVVqUVRKrWozfTKvLwxXfV079Ws1aOa6ifm3Knq\n+4H3v+UdBE5ON015e3NtpnzS4v4hq8Jms2Ly1O5cENp3h8oXODla2ey5+9PMfQqBZUx0saNLS8b3\nfBEvnV5jBM77HWe7Cy52W1R6qANImQeloKxWK3MOdX8tZtptQDsnNsPV/na8WUJVVCqKzMvVoSNb\n27FBODleUkrFeeFR5Z3g1Zs0xVfaLFCl1kopSm1u9WSVJ2WuN+sDh26aS83aTgcVbeYSjo/XTRNu\nPuf0EefoYmCdOjbLFZvFhndfe4FBPINzXL84p3jPoFAqyFjMNFPsUArL1dIcs+ns7YKdg+Bs1leZ\nfALlZLMCrWjdW7nJo2+3xLToCcLJekEthSqOvRt7OXkSAM57Maj2UH5FVb/ygTvOMZuFPKrm6Nia\nPJnKtmnzokVk/tt+2pvZVoVKparDoRZ2ibzheTkEL47kPNEFkg9EFcKY8doTnUecZ1mVtY+M3RJy\noYyjefsoWm2hmcK6aaZ7J/jmOCJmyVQrtVZz0ji4PydInSKNNtgPBkBByQKuKIhFLI8iT2rG/xLm\nxU+POb/pHgfuq7a1bHLOhGYum1dcpUXIUzwu5iM4175vx6hA1Xo4sdv6vj+nCARxpvT2iQp+GPFj\nQZzH+8BSYe0CY1pQhpEhDM3Jq2ZrdH88h8M7CE6Ivtl6mkInxbNXOm39r9PKMw2ovTdLVci1Oaai\njOXR3Ponpfj8IEfuXjLN8PmGdZ7ATfH2cAwIEVurm4YNJLG4G2zuaTXzKHVy5dTCt3ldB1Qt9naB\nzgeS8yRxhAqSx+ZIOZxzhBBZhMAqRPqYzNvXStFCVTmIE/YmfrJEOvkqqm0Nr/ONabNCMod6zANg\nHlCY4ktVRqCWd67iv0xEPoWBC/8X8IOq+okH7TA9nHpws4emXCalH4yGydGdPNwpvnaAti/t75P5\n3cd7kxPoxRN9IPlEkkBUhy8FHTM5Z2gD0XUL0nLFMkS2IZAWC5QKNUPNzRO3C59CvVynkK3F9OiM\n3Nml7Jcocz0EFcO7GyJkk6FFOBXIValicf+jyJNQ/IeA7wF+FXgX8MPAz4jIV6nq+X33as5N1f0M\nN1Mo82wwxcvsvE0gjs5PTmY0rdbm1DmxNX5yAWV/jAn9iz7ShUgST9S2juZM6ftZsUIlpUSNkUWM\ndECtGR13VOcQCjQlARS1mb4HlOrstAl2kTo7qm3A7s1FM2aTM2iDqlTbp6KM9R2meFX94MGvvyQi\nPw/8JvAdwAfut9/Ln7uOa6Z6WoVPj9dcOd6YszabxulEbYaoOwT4mIbC7BjU+brmmUVb1714umBm\nvvOeqIJXkGLOm46DKb0WGAf80BO9ZyGOdUpo7iixY8zmwpiTNylTKWqDS5sS2xA1x/QAnyiToyau\noYk2yJ06oHL79gXn5xd3LH/1nab4u0VVb4rIR4EvfdB2zz93heUimTcsFvOaZbfQZRrpk0ymeoZe\nHfY0GwYwbSROZmdpmkwWV5snvwiRLng654kVfFW0FDRnyjAgWuwzDrhhRwyBZbcgp46SM7s+gR+w\nECTPVsjunTtRpEPlt1WrWhxHVfAevHe4NsDVCarCer1mtVruIWhgGEZeeeWhs7GzPHHOnYhsMKV/\n5oEbHprBeWZKi8Hbozx0zKb49tBX4wDyPPh/VZt9pU6IoFoIFyLJB5I4IoKrFc0ZLQVqQbTO2Dil\nwNDj+h2xZFYiLIKnS4mYOpyPIG6etZPDNt3L5KPMy1a7BdXmAxzCtYfI4kFIJ9pyC83RfRR5EnH8\nXwD+J8y8fyHwnwAj8DcetN+EVUzhfNXmpE3zR8xsCpOz05SOcPgMtD3NPezbEiW1zTTztnBBSD7Q\nuUDEEUpFcqHmEcqIaG1rLuYp1kode9Q5YpcQLeycZxE7+q6iJZMHZzi8OBssb9CN7hf2hkVMN2eh\nHri2ltl1TlZjPxSkWcPq3mGKB96NEQOuAZ8Dfhb4fW/KEtlbsTmkbxHyHenW/fbK7BEffnWwdtbp\nZzX8e4qmvRMcjuSiATcIviqUTB0HpBbLocfQwsxKFSBnkB1+XBFKYSnCIiZ2VSlDj3PT46zcnUTR\nSektYXRH2gHm76brnECsw/XCNRDLOSF4/1b1cU95Es7dH7vMfq557BN2Q3OM9hZvdoFnT3iO0HXy\npvcLqra4dzahCh6L9aPzDaxxRBFiVXxVSlvbnRNStyZ4TykWe+c8UMbBLE0ecbsLUkisfaR0HYwL\nau6pZYQ6olo4TCTto9QpUdP+f5DkMd+wNodwzw2YcIrQUEDxU8h4eXnHYPXiPDhHbSYZlQMzLvND\nBN7oMKkhdBYfN3SvLctlmulqjp4XT2wzPYkn4Qha8bUYDl4yMXSk1ZpuuWLMlTEXdHdB0TMYB2Qc\ncbstaelYpwUaInUcGPNAzWLZtjpxAmrLset86VPaZbIJk8mvLR8/rfLTHyfzHoIjBvMhcnk09+yh\n9xaRbxSRvysinxKRKiLffo9t3icinxaRCxH5SRF5oEcPE449fabj7G9aDpy6aQjMiZxa5/Vc9fD/\n7cG3DJcXoQuBdUosfKQTR1DFlQI541CCD8TFiu74Ct3VF1k+/wWsXvpiltfeRVpfwcUlTgXpe8Iw\n0JXMEmUVI+vliq7rCGEibcger/eNYNI+Ex1nus4pC+dE8U7xHoKHEATv7WPPAZCDSXBJucyMX2P5\n9f8G+Ft3/1FEfgD4D4Dvxvhg/ynGqf8dqjrc76DmeZtNPoQ8LTnjUGrL3LF/YHCX+W/r/gSTNs5d\naFBodI5ljGxSx8oHEkKoFckZzSNeBJ8iabUhnVylu/oi+IS6hH/tc5QCNVc0b9Fhhws9KQ+o9+Tg\nGd0KIVNLTykyg0YWnzlULJdn7JqyB6F09vfwYskdCxBcu4+JIGKADqozZnBZeWjFq+pPAD8BTJSq\nu+X7gT+rqv9z2+a7MYbtvwH8zfsdd6ZKzZHuhK21iF0at00b6WKGMg9i+8lhnm2CGVQnnuAcnfcs\nQ2CVEp14EoLX2taEgkuJkDq6zRHp+CrpyvO4tELiCpXIsN2St1vKeaFuz3HjiBsGCIHRB8YYKaVj\nGCO+DJZgmmM0ab7LNLjvSBe12S42y30L/VocXw8GxrRuPeIS/3jXeBH5bcBLwD+YvlPVWyLyDzFO\n/X0Vr4d30gCX2hI3DsV7IbiAiOHVpVRqqRSqhWoHsf+U4pzWsegcyxBZxsiixe5RBaeKlGqzR5WQ\nEuloQzo+IR6fEI5PcWmNS2viMJDObjJeXDCUgXpxBrUiQ4/zDr9YklJHiomYDGyROuDKMK/n0ihW\npagBVN61FLKZ+RgcKTi8F8S5htXrzMiZIkEnjvAOC+dewq7tXpz6lx686+T27uNYw+1tigfviCHg\nvOByZZRKppgplAPFN47C9LsTJXkz8auYDKnzgVAqvlakKqUW0IqPibQ5aoo/JR6f4ro1Lm2I40g6\nO2M4v03Z3mIUb55j39tMTR3Je1JIpEnxGRyFKpZxK6Ua+6a0bCJm/uVA8TE6vG8+gICKUKVRsrAJ\n4cX8hkeRd4xX//qr1+e06uTIHx8dcXKyse8EnIcY7MEkhb4f2JWMNjM5p0Hb9gbXelKILFIylM0H\n/ITSjSNSi8GkoSOu1sSjU+LRKWF9hF9tkLRC4pKwPqI7vUbeXlAvbjHefB0dt5bEGQf8OBDzSEJI\nIbUoI1PKwLTkiJiTJ9DStIAqjopTJQRHaF77bCWA6zfOePX67YMgkD2+f0l53Ir/LHZdL3LnrH8R\n+McP2vG5a4bV+zscO1PjVLjgPYRonHsvDtHC2Ft+eoIyD1Z8I2d4SCHSpY4uJqLzBMRM/DjaTHeO\nmBJxvSYenRA2J4TNMWG1gbCAuCCsN6TTq5R+x3jrdcL6hHxe0d2IDAN+GIjjQHKOLkSqQJ8HCjuE\ngujEtHEEcZQDT97jcKI452xdR2YKlojywtVjnr96ZMSSNuO3255f/MgnL62ox4rVq+rHMeUfcuqP\ngX+ZB3DqoYEU00+hpVL3qNzs2LjJpOuclDlE7OxCzC0MztOFOH+SC0Rx+KpIKWjJoIpPibhaE9ZH\nhKNTwuYYv9zgFkuk65AYcYsl8eiYdHqVdHyVcHQFv1ibGSoFl0divyOVQidC9AHnA9VFqvg5Mp+K\nRbwzy+W9wwdHDJ7gHcH7VlfgcFh9gSWs9hGOtBzGo8hDz3gRWWNJl+nU/5yI/E7g9Ua2+IvAD4nI\nx7Bw7s8CnwR+7MHHbUmNBm9MMx6YzV4BRq30JaNV2Y0D4wyUKG5K5mMhUfIt5Roal06EUBVXqvHY\nS4UUcYsF/uiYeHRC3JzgV0fIYonGBOKt8iYG/HJFODomnpySTp9Dxx1ld46WEZcz7C6IqiRxjCL0\nPuLSAs1Qs/kRk02aCi6kJZoqlpX04mw2uubpIwiGTRiVTM1neETNX8bUfw3wvzKBUFYHD/DfAf+O\nqv6IiKyAvwycAv8H8K89KIYH9jHv/DvzA4FJ+cpYK2MeGcZMHouBN9CUrrNj50SI3rNMzaFz3pIx\nteJLsTWyVnAet1jij83Ex80pfn2EdEs0pNlvcDGiKwj5iHh8hXT6HGV7xnDrddjexuWC214QxJFC\nYojRopC0NFApD+zzNnXOIJrxUqoIgSnrJuAmc1xbBKeNLm7/yiUUdyiXieP/d95kiVDVH8aYN29Z\nJj6aoJaHpllzmjWoBRkVV219dhU8DvFToubOlGf0Vg6VQiQ6Y9b4lnYto1GqfAyEpTlupvBj3HKN\nxAU6JVxaZCEC3nu064ibIxZXr1EubjKcXacOW6T0MPa4cSTmkc47knMk35FrJpeh1ecV+wAT4lB0\ngnTrjGFMJVQOxTOFc83Tr4fr2uXkHePVT2REUJwagoU20gSKK4obLRSSNrl9Y7POsbvITFueFB9D\ntHy7Cr5UyJk8DgQn+BiJiwWxre2m+A2SFqifHk1j+oog3hM6C/nqlWsMt28Sbr5GPj9DtwXd3caN\nIyGPpBitBi9GtIzUnKwEqzQYeTq6GnX6EHKeIhNLLJmDpzN4cyfOf1l57Fi9iHygfX/4+XtvfuTD\nDNbk5E1Yt8NhYIYWNf65GiTq3N75cSJ47wmTwr2BNaE5dK4WpGQoGecdYbHAr9bI6ghWG2paUJyn\nAFoMxiWPRr/SYmuu8/jFwhy941PS8VXi0amt5TikVAvtxoGFVlbOsfBWhOlDRJy/pzM68QdyrRS1\neL82oDq3LCM6VefOjO1Ly2PH6pv8OEa4nC6vfysHnga1RwgiRGflxcE3SobojNc7DkK3CbBB8c7j\nQyCGZGGcj8TSLEYpaCk4LWbmVyv8+ghdrcmLFeI91EoYB7xWfBlR51HvEbGPLRGRsFyTjk7oTq9R\nLm6j/QX57IYpZxiJbscyRFubnaOGZIWXpVBosb1Oubrp7tnnIJqzW5oldICrxtVXhCCPFpA9Cawe\noH9YXv2Ugp0qZiZ0KnpHCJ7SZsPMp9s78G2Nt0jAN4QvTWGcD4SScbXh8W3m+hgJqzVuvYHlmpIW\nII5aMnXYEbMD79EQqCFaIaULCM6KLNoS0Z1cJZ/fJt++CXEBZUDGAS/QdQtcrVRx5BjJWqh5mGHa\nQ/O2zz1yOKRt1tfaeIgVr84o2O7zrPi3KN8sIi8D14GfBn5IVV9/0A6m9D1jtmKIqFODNOvBB/YU\na9gvC04cyQVWPrHykYU4YqWFbwbWiLfuE26xQtYbWCzREGydzQN5e84wJVBEKN5TfCD4SHSR2LCA\niAPnbeYfnzLeOsVvTqnbM3Q4R8cRN46kPDQnL5BjQnOi5mS5fzKq5SAldUCxPqRm6ZSGluYHyTuS\niPHjwP8IfBz454E/D/w9Efk6fVCLrQMHDdmnaV21UK20EGY/I+7MbTkxNK9znlWIrHykwxFVoVq+\nXbUi3uFiwi2XyGoDixXVRyNCjCN1zNSq5FrIKH0rmgw+sgyJZeg4ih2btDAUbrUiHZ/SH5nitYzU\n/hzNIy4PpvwU6WIiO6g5UcbOzH0rp5pMvmNfJqZt8Ff2/MHaaFstM/tI8iSoV4cZuF8WkX8K/Drw\nzVj8f0959bU7efUicHq84rmrx5SWoNjf7WQbpa33IDiC+Ear2pdCuWJpVy2lATEJt1xTuxVjWIB4\nA3KG3tCy9one4xtrVjEOvhXOZIYK25wtySMO1y0I6yPSyTUYe8bdbbTPMBqo492KFANZHNkHfExU\nLVBmV52Gz87DudaWu6/Ka6/f4sbNi5lcirzzsPo3iKp+XERexdC++yr+6pVTukXaQ7YOvLOOF64e\nmHUxkoITd8DWaTPeebx4C/MwS+FKMdSsZMQlJC1gvSGnpcGpVSjDCArLGFmGSEqBsFjhYkePY8Ao\nYTSoNw8952W0QeYDEgJhtWFx+hy6O6ec36COPeSCXpzjvCOlSPGB3nkkJKvNw91JI7M7NNqYqqWf\na+X4eM3xyaaxcByCY3vR85GP/tal9fLEFS8i78YYtw/k1U9NEOaYpdrorq59P5kB9qN+bg+iBnUG\nFwjiLXxTaZSqYpUwKHiPdAtYHZHjgp7AOBbG8cLSpikhKeG0EkIixA7E4V2gYJWxlULJI32/hRjx\nzhFiwK/WdKVQzm8y3nyNsj1H64DuRlyXSHlJcY7oHD7YIDCajcz3Yeu4mfdcqsHRxe7/kIQh1M9/\nJc2DsPr2eS+2xn+2bfefAR8FPvjGox3Kniuvul/jJsbNvtpN3sg5d5aQWYREcoGgMidiyKPtGSKu\nW6DLNXW1ofeR81I5393m9sU5u+GCtXesgme5WLHcHNOtjvDdEr9YkWJHDIkgHhUleNeyaRZNhOUC\nL0I+O2U4ukLpt9TdLcq4Q8aMGwaCtDDVB4qzJI7WPIPfqkpuvk2u+7q7mZM00bJF76gquow8bqz+\n+4B/EePbnQKfxhT+H7duWPeVmXaljXzQjl7V6EeH698c0TVv1/LcgS5OhY8OV4slTsYR8W5WPJPi\ni3CeC9fPbvPa669wdvM1OlEWUlksVizXRyw3x6yOT1kfn3K0OeF4fULqlgale29MGVHwjrDocGlB\nPjslHl0hn99mHLZoLsgw4vue4BwxJkIIeO9xzlNcmKe65fCrETbq1B1DZz9meuATQ+lR5Elg9d92\nmQuZ4tqJUTulIy2skXktnKwCqm0bR/KOzptHH9VIFpLNodOSid2KsFzhjo5hc0xdHyO7kVoGxlrp\nh4Hz7QWDFnZaiNsd6eKCdHbG4tYNlqsNq9WGzXLNslvOKOFqvWK9PmK92rDo1qTFmnh0hXRyRt5e\nUIYL9PYNpIL2A+IcUTzL2FF9JMcFilKzsYAqzD339u3aDmzdhOPeFz556/KOwuphT6gwGrLMbJTJ\nFE5KR8BJIDpTeOe8VbxmxZdq+famfBcicb3BH52gRyfk9TGi5zBYZm8shWEYyWWkLyNOtriLc7y/\ngY+JEBNdTCyisXhSiqSUuHLlGs89/yIiaoNrtSYdndJdMVLmeH4TQmf4e98jAjF2LHEUH8nRFDzW\ngjLMiZmpDUrrimbPhANuAlMK+/LyUIoXkR8E/gjwFcAWI1f8gKp+9K7t3gf8u5i5/z+B71XVjz34\n6Nr2bWnVOaafM9iznZuMfhCh84FlSHTOih+9Flvbizl04izciptj/NEpdXMCq2NL+GxHGxQxEdMC\nKQ7JDq2FnAvDmGHXc8hziz6wXHTWUVMzy0Visz6iUnEpEtZr0skVxotz+luv4hZHkC/QvIVhwI+Z\nRamMOIaQyFopucfaLtY7mhoyKVymSb4Hdx91zj/sjP9G4L8C/u+2758H/n7jzG+BS/Pq90Tq6cam\nihqd89jzx9LVRB9YxQWruKBzkVAdUkY051b/5pGFhVrx6BR3dEpdH8NqQ+xHQjewWh1x5erzdCGY\nX1AtXOuHnr7vGfPAOI7Umsm1UDUjAyCVYdiRx55aB1QzKgW/iHTHR9T+KuOtaww3niPffp2SB3Ss\n+GFA+guSDyx8oGhk9BEkYIO7MLmyU/Cyr4zdw7zyaEv8wyleVf/Q4e8i8j3AK1ib8p9tX1+KVz+N\naplYKS0degDNzwCGa4pPLrAMHauwoMNCOIpCHtFacTESuo642lja9eiUsj6G5YawHQjdjuX6iCDC\n8WpNUMWj9NsLbp/f5vb5bbbbcy4uzunHHbn5DCIV0czQbyljTy0DVUdwBdcF0skRmkf6G9dIx9cg\n99SLm0jukWHA7bZWrZMWjIB3CSRiHKN9yDoN8ImkMheUyJ14/mXkUdf4U0wnrwOPxKufGKazU8d+\nLbtz9Buqlrx1s4iugTW659FpKdawaLEgbY6MUnV0gt80osViyWK5Zr0eiOKoiw4Zj1jGwCIEcs5c\n9Du2ux23t+ecX5xz++Kc8/Pb9P2OjkInlZOTF1hvrrBcHJHSCueTpV0dhM1IOj5lcfU5GM+pFzfJ\nW2uHSr+zDGLtSAIpBFK3YKTOPECV2YefPfg5ijmoxbusXFrxLTP3F4GfVdUPt68vzasP3hOCb3nm\n/Qo2edDTI3DTOpuSpVzFN5SutuKIbLFxSPhuQTo6MZ780akpfrUxxa9GNmMmh4gvK6JWjpZLjldL\nVIShVHY5c3Zxwe2Lc67fvMlr129wfnZGKgOx9Fw5ucpmc43F8piUlviQEAkQHZoL6fiU7spzlIub\njDdfpY5bI2PstkbwrNl4gTHQ6QLqSBn9nLixWzazN+P2GDOpfr5LqA7k/cBXAr//ka6giTssCjwk\nKbRFfU5MiLOY3U+sWSHoBNZkpFYrVoiJsGx06fUxfrXBdSsIEXWeFBOrxZLqHb52RFGOliuOVksk\nBDLCoMpy17PZ7YjL67i4outukHJPHHccn5xwtDlluToidivEN8WLwy0WxKMjFleuUc5eZ9ycUoet\nhXhjj4wjYRxIInTOsUgdmgdK7BFqK/ac+uUyM4se0cLPcinFi8hfAv4Q8I2qegjFXppX//Ir162n\nO/t7Ozlac3pytP+m5et9U350bm5qICVTh8H46TEal25ja7tbHeG6JeKjMXdyJVlsZUWI1Spl1TmG\nqkhVqr1dgBQ7RDzjkBmGAaeFmCMxR46Pj9kcHbNaHRPSElxEpa3KIRp+f8WcvHTzdUpvM77szpGc\n8f2OKLAIiZoS5EQtC0SglNHKrVW5/vpNrt+4fYfOa3k0uuVlINu/BPzrwDep6h1ZgpaQmXj1/0/b\nfuLV/9cPOu5LL11luezage506moL4SZCpW/tR4NYcYTX1rtm7JEuEVIirlbGpduc4JZrI0m4YMfN\n2axE8FaeVMWYsCL0tVpGbyJcBGtumMeRcewJFEKOxJxmxS/XGyQtwTX8HZBooZ1DGW5eZbh5jfHi\nNnl3bn2Sxoz0veEQIUEMlJTIpTOIdlRGKlIr166dcPXqMYfNjy62PR/92KceVn2zPGwc/37gjwHf\nDpyLyPQWipuqOr0p4ZK8evPe7sy8ypytm1qKL0Kia8zZ0Ga7K621qGoDa9bEzTGy3qCrFTsRtv2O\nWipVrUmJlmK/10KtGWo15o73iA/goyV1GpCSx5HkPW61IuqCoIXV5pjYrAkh3IGoiQguRFgsCZtj\n4sk10vm4N3ZtAAASuUlEQVQZeXvGeHbdNhpGI3CmkVo7OucZU9e6d1tySQEKOKzRrcjUC/DzCOAA\nfxKbhP/bXd//CeCvAlyaV98SVVX3YdtMRsGKJhet8NHaj4bWwqQal67aqx18jBa+bY5w6w11uWYn\nwq7fkvtdqz5tGHi1OnPrclVJwZOCpVnxEdxe8Q4lec8yrImtR2232pBWG8sB+NiSbG3kOsHFhIgj\nbI5Jp9fI52eMt64zxCVae3TorUvHYknSyugcXerIKGMxoqc0D9+69hyGu48mDxvHvyWi12V49bP3\nbq+UmPFp6wFsxRGdj9bJwnmiGCZPbqxZLAMXuiWx8eTd+hhZbihF6WtlLGY6per8+KoquWTrfaOe\n1JoeV+epeEoxFC8JLKaSJ+eJIeKdt4RSVZRxpkaLWG5BBMRbbj8dnZBPrjLcvELYnFK2Nyl5sE4c\no9Xf2T0GckiMITGE0ZC8UvckTGaM82Ee7xvkHYPVTzTqGZ3SSfENKnW+daA0aDYouFrQPFKnuD0E\nq3jdnBA2p8jqBF0eo8NIGUeqZoIUvFgFjTrPOAo1j2StiAqdFpxADoExJHal56IMjFqpGQiOINB5\nT62VPI4UppchWV27OG/EECuHxKdE2hxTTq4wnFwlnlwDzdT+3BQ/DNBvid2SEiLjBCPnzFgqpQ3Q\nOrn4jwzfvIMUD2/EoI1D7gjOGWDjw74USmmJmNHW9hiJ3ZK42hA3p4Qjw+R1uUFkB+oQdTjNeCmt\njWSwEiyxt0NQBV/NykgIaErImCniyLWStbQXJ1hEXVuzY21NiotWXEu3Bh/AJ8Ql8ztWG8rxFeu0\ncXKN2p+Tb19HxtE+ux3BR9JCGF0ghUQfCzlnlN5Kquu+1u5R5bEnaUTkA8C/fdeuP3E33Hu3jGPB\nuUzRZvaxjhHeOaKPzYt31rKsFUe0VpX2oLtWzXp02mL3EyNTdktW6igEsu9x4w7JrZN0ybiSjZAp\nQhVh2+Lq5ANd6khYZyspI6GO1iErBopzxgOshaKVXR7ZtQZKTiCGyLpbserWNoBDIC5XdEenjFef\np27PyGfX28uNFHY9Lnb4nM2RdYGQFmYNnAdp/XL17Znxb5qkafLQBRXDmMHZ+6F8exUYgGvrafCB\n4Pzc1MC1XmZa1apiuiVpc9LM/AmxwbN0S1Z4cIHROSqVotnIDiXjayEcKh5ribb2gXVMbGKHIpQy\nUMad9bED8uTI1cJYldv9jrPW7dppYRECbGrj5Vlql4WlbRe7HfnWdYblEbXfoWWL5gFZDIQ8EqLh\nFEE8LuxsWZLGR2u+w+eVZfsWkzRwiYKKfQauzfbm7MX22pAk1oEyNAKljrm1HXW4EPApERYLXIw4\nH6zgsmTcuCPkka6MSOkZc48Ou1YilQnFyqPU0dqSCUmUWAuhjEz9aSvV6NHKTIcyxVvVbVcLWQta\nM1JHAgUZzqk7R/EdLhgyJ1RC8IQYibGjhkjJO+qYccMIu56AMXWS9yQf6UNnZd2Nk3AQzl9aHmuS\n5kAevqCC1sFCpo4X5tgF5+aGBpE9PFvHwTB67/Ax2iclnPet6fCIDFsDfPJIHDM6bKn9OaU/R/KI\nNCAnTW+h8Pbeu06UVAZ8f2EzzTnQStDSmq613nlqPLhSFUchun3ho9NKHLfUbaW4DnwHY4Hc41F7\nwWEjXSpCLRU3tLW+3XPnI0OIxLhAq701M0thagz1KPK4kzRwyYKKKe+8f+ujtR495MlHZa541XEE\n54xPp4pW60pZ+x354jYgyO4CF5K1Ki0ZHXp0d0Htt+YUltFeRuQD0Ud8iYQc8XlEh57sDYJVcVSZ\nMma1IWjV4nsFqYovha5kahnbclCp7jaDD2QXcS6ab7HbUbc7K62u5aA7djV+YL+1ws/YkbDWLTEk\nSs4U34o4D156cFl57EmayxZUvPLqDbz3c1MDJ453XbvC8+86nrl0vtTGk7eGQ1WE4oRRQG56tFaG\niwvCrRsNmw/ggqF61eL1PPbWl7YUai3G5BZHcb4RKB3OBbwLuClOp/Wfndqna2tKOjUrUCNK2itC\nS2POHrwDxxYvWyfGER1G8q3rjLdukHfnlLE3Jy8P0FtEEcaOmCJRlVdfeZnPfPqTlJJtO6zv7qPI\n407SvEHeakHFiy9cZb3q8N6aBy1jN6N0nfOkooRacbmg40gdRzN5DnLNLfmxxR0o3WarIK24XBuL\ntdbSIF5zlozYcdhYYWK4yR3p0DuewZRQmNf7/TnmxgXWhXG2EBOpTlRhHGAc0LGnjoMhj6NZCqLH\njwtS7gjAu7/wi7hy7Xm2F7cYdueIFrbbCz7265/HgooHJWnus/1bKqiYMm9T06LpBUHBGQhCzZaR\nynl+x2sLbYEMfU8tFRlGZLsFH6jOoe29cThrQLTn6+vcXGBm/szXMClyKl+Yah6mvxwQxeTOnzO4\nWStSx5nbTx6gtJcftA7LUtsSRN13u1DDCWqzbILDYw0N5164ai9aeBR5rEmaVmzxXi5RUDF58VY4\naM5N8hEvwR5yNcCEYkgdU1cJbTOyDjBmtB9Q71pde6CGACFZHn6yAq3GXEVmhTJRvw4GoGDLgMoB\nQt5KWqakku1+wBZs7SelFNzQ+uDX3tbvbL3wpbQXFE61BM3sVPatzsq0pLlgHbJaOto7c14fsUr6\nsSdpCpctqOBOjoGVTLsZAjWkzVmLkhiQmvZ7tHWYpkgz6e3/zlnSJS2QmFrmbRoArW2oORYHpn6v\nUNe8+nmgIPO1mY73+WMji5hJkVxw/QXSb1EnQDXnsDjE2fqshyRKsGtxgqYObY7lREKZnd9pUH4+\n6dVvlqRpqdlLFVRM72UpYiM+a+tT61oVjJipMyg1ol037yvzTHRMr131PqBpgaYOv9jglxt8t8CF\nhMRkmTdnsK0Eb90w7sKMrSedty7Sjf6ltO/EU2plrJl66J3XpvxxpG7P0d1t6vlNyvkK7S+sHUvN\nzOjbZDWm/4hYM4aY0BD3fe/tCTNZiAdVnL8Vecdg9fYedRA31Y4VslaqEyRGxAdcCLiUqHmklnxg\nIibTa40VxAEhQrdE0oq4OSGuT4iLFT4t8GmB86EdM+FSsnZm6P5dcBPbx3nrdoU0J09mj38omd3Y\nk0s2016LvWigQh0H8sUZ+eKM8WwJiwV1d46rGan54NIPPIY2i2sDjRBBxxHNw4FF1Pnfo8jDrvF/\nEvhe4EvaV78MvK+1R5m2uUQxhd1OrdW4bjmzHfo7nKjknBEvHGjwqHcz595OjJnmyQmKCek6XNfh\nUkJipDhnufdhxyJ1VnmTAt1mTVquzaS3NXxa8OceuXNxh8xKyRfnjDlz3l8wXtxm3N6eE0pOlZJ7\nilR2WtiWgTH3SBmhDLZ06LS0NH+C1vpEhCxCBrbjyHYcGEZDHUstqBqY8yjysDP+E8APAL/WHvX3\nAD8mIr9LVT9y2WIKmGJh62o1HCYkBFQqKaSWAw9mlmn18dMDaNh5aLF4SBHfJUJ7R5yGSBXHWDMl\nj1TvCVJxKbBYr1mfXG3JEN9YIVNIN1Xxmr+ByNyo4bzCcPucs37k/NZNzq+/wjIFNstECqHl6ZVz\nzZyVkX4cqLlHc8+8ruj0fr0p0tAGFtk5dsNA38q7arFlRWv5/JZJq+r/ctdXPyQi3wv8PuAjXLKY\nAuzGb52dc3y0spf/lEwv4EZr7R1rJpRkJrqZwk9++jN8wYsvHgD9SqyeUB1RsBbidZ+I+ZVf/ke8\n58v+BcZSCGOgy4FlK1ZkcgJ9bMcXfvYf/F1+/x/8w82L9vtzT7G6D6gLVBxFhY9/9MP89q/8anJR\nnKtzh+ptyZyPA9thRxl3lHELKrz6udd54bnnmCAenV450jp0VpRhzAyjhbI3rt/g5GRz0BDi8vIo\nkK3DXhu6An7uUYop7HiO89sXnBytgdb8SJV+HC3PnUfE9fbgW9jz8U9+gm7VMXNSRIneuHld17Os\nsFAxrD90/OpH/gkvfMmXMZTCdvBE74jbC+Jui+93uKr4OJl15Wd+6m/ztd/0rViH6UAQEPGtR46i\nKjgfiWlJt9jwmd/6GF/9L309MQbEKbVmxjoylMJuHNj2O8b+gtxvQeGzr7zMarmYPXbZryLzmp5b\n+9WqlRs3brDZLD//2Tl7uPJV2BuiF8AZ8EdU9VdF5Ou4ZDEFtE6WctC4WK0NilIZ69i8ajc/EFUY\nS+bG+a0GjwKY4qP3LPJIEY+6gMYl0hlhY6yVUStDKfTjyG4Y2PY9YdcU36BWMBClHy4QEYKznnVO\nArlOb4pWvPek1FEXa7yPLBbHRA9ooS/bWfF2rp6h3zHuLqBCKZnzi/Nm6ttLh9y+85diDm/RfZ18\nrfWRQzm43Iz/FeB3AifAvwn8VRH5A496IS+/ep1hGPn0Z14FzKM+Odlw9cpmX2TRaFk6/7+lRZms\nvWK9IMU6YKp1kUohsuiWeO+5cnxKQVq/m0RIKyqObT9ShpHKbZQKWhiGntde/RyukUGSj3gfGm3L\nQR5IrrLsPH7VEYNns1kSRSh1ZKBCLaiL5ju0XtQTJmg/aiOYyv5d9xOY0O7r5s1bnJ3dZrfr+dQn\nP2u4xdvwMqIM/L/t138sIr8XW9t/BLvchy6mAHjXC9f43Gs3+KL3vDgjaHcE1QDzywMn/JsJs8Wc\nsMlJLGix98J6EWIILLoFwQeuHF0BH6xky3l8TBQc236gHwd200sDNdMPPa+99ooVb/hgPe6C1cbH\nlBCU6JRV50maCMGzWa9wwJgDF8X4/riAHih+6m5hz/Nw4LpZ8daT3hohHm82bNZrPv2Zz/LSSy/g\nnTD0A7/xiU8/pPb28jjieAd0j1BMsQAY+pFSKrvtMEdT808MFLE8x/7VY6UWtrvdHZDflMevFcRF\nqjoGEn0R+n7H517+lBE12lsuDBG0lwnsxh390BvPXgu7i3M++fFfw7d+uMG3HEJKxBipTijYklSG\nnn53wcuf+U0E4+Hf3O64tb3g1uuf4/zWTbbntyn9BWXo7bWltdD3+2BnSkvPgJRYz5vSEkG1VvrW\nYCGP+Y7n99CyfzHfm3+AP4fRr74Y+Cos156BP9j+/meA14A/DHw18Hew0C894Jjfxd6XefZ5+M93\nPYwOp8/DzvgXsCZH7wJuYjP7X1XVnwYuW0zxQeCPY3H/7gHbPZM7ZYEBaW/STezeIo+K+T6Tfzbl\nsb6M6Jn8syPPFP+UyjPFP6XyTPFPqTxT/FMq7wjFi8ifEpGPi8hWRD4kIl97n+3ee48XHX344O8P\nfFFS2+Z9IvJpEbkQkZ8Uke980D7yxpcrqYiMIvKyiPxtEfntDzjHICI3ReTsfts/4Pg3ReTnROTb\n7nPs6fq/9GGfN7wDFC8ifxRrhPxe4HcDv4jl8J+7zy6/hMHAL7XPNxz8bXpR0vdxB543n2viC/x7\nwO8FzoEfxfCIe+7T5MfbOX+67f/1wL8CRKx2cHmfc3wI+BhWUfRt99r+ruP/W8B3Yo2if08734+J\nyO94wPV/UETSfa77/nIZ1OdxftrD+S8Ofhesdcqfuce27wX+0Vs8bgW+/a7vPg386YPfj7Gq3+94\nwD4fAP7Wfc7xXNvnG97KOe6z/X2P3/7+GvAn3sr1P8znbZ3xIhKxkX2Yw1fgp7Ac/r3ky5pZ/nUR\n+e9F5D1v8Vz35AsAE1/gQfLNzVT/ioi8X0Sutu/fUoPHg3M8sNbw8Pgi4kTkO3kTvsNbvP43yNtN\ntnwOe5nivXL4X36P7T+E0b1+FYONfxj4GRH5KlU9f5NzXbb54n1rAXn4Bo9vpdbwR4F/H7MMj43v\ncLe83Yp/KFHVQ1z6l0Tk54HfxMzoB57QOe9XC/h3eLgGj1+DDfI3qzX8MFaA8n0YFv9Y+A53y9vt\n3L2KFWG8eNf3L2KVOA8UVb2JPaS34tkeNl986HMdnPPjWDLpG4Bv1vs3eDyUb2jf3b39vY7/Mey5\noKr/Eebsfv/juv5J3lbFq1XX/AKWwweYyq+/BWuz8kARkQ2m9AfX5TErbOILTPtPfIE3PdfBPh8A\nlpjz+YYGj/c4x1/GlqUfvHv7+xz/7lrDme/wOK7/8GLfbq/+O4ALjJL9FVhK9zXg+Xts+xeAP4Dx\nAb4e+ElsjbvW/r7GaGG/C1sj/8P2+3va3+/FF/gY5mC+YZ92vB9pD/eLsSaNGUshfyE2214EFgfX\neHiO/wEYMFr6u+/e/h7H/2sYte3X2vU8Mt/hvs/97VZ8u6Hvaw9zixE5v+Y+2/0NLNTbAr8F/HXg\ntx38/Zua8spdn//2YJsfxsKiCyyX/V332wfLef8ENtN2NLLXPbb97ruuczrHRJa45/b3OP6t9tm2\n7/7+pPQHXP+XXuaZP8vHP6Xydjt3z+RtkmeKf0rlmeKfUnmm+KdUnin+KZVnin9K5Znin1J5pvin\nVJ4p/imVZ4p/SuWZ4p9S+f8AbAZa0uugBIgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d474d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index = random.randint(0, len(X_train))\n",
    "image = X_train[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "print(y_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Shuffle the training data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup TensorFlow\n",
    "The `EPOCH` and `BATCH_SIZE` values affect the training speed and model accuracy.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION: Implement LeNet-5\n",
    "Implement the [LeNet-5](http://yann.lecun.com/exdb/lenet/) neural network architecture.\n",
    "\n",
    "This is the only cell you need to edit.\n",
    "### Input\n",
    "The LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "### Architecture\n",
    "**Layer 1: Convolutional.** The output shape should be 28x28x6.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Pooling.** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2: Convolutional.** The output shape should be 10x10x16.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Pooling.** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten.** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D. The easiest way to do is by using `tf.contrib.layers.flatten`, which is already imported for you.\n",
    "\n",
    "**Layer 3: Fully Connected.** This should have 120 outputs.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Layer 4: Fully Connected.** This should have 84 outputs.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Layer 5: Fully Connected (Logits).** This should have 10 outputs.\n",
    "\n",
    "### Output\n",
    "Return the result of the 2nd fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x):    \n",
    "    # Hyperparameters\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Flatten. Input = 5x5x16. Output = 400.\n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "\n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(84))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "\n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 43), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(43))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Labels\n",
    "Train LeNet to classify [MNIST](http://yann.lecun.com/exdb/mnist/) data.\n",
    "\n",
    "`x` is a placeholder for a batch of input images.\n",
    "`y` is a placeholder for a batch of output labels.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline\n",
    "Create a training pipeline that uses the model to classify MNIST data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rate = 0.001\n",
    "\n",
    "logits = LeNet(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Evaluate how well the loss and accuracy of the model for a given dataset.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "Run the training data through the training pipeline to train the model.\n",
    "\n",
    "Before each epoch, shuffle the training set.\n",
    "\n",
    "After each epoch, measure the loss and accuracy of the validation set.\n",
    "\n",
    "Save the model after training.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.609\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.791\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.852\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Accuracy = 0.880\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Accuracy = 0.902\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b194aec201de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_operation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mvalidation_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EPOCH {} ...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation Accuracy = {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-03f1bf2fec56>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(X_data, y_data)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_operation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtotal_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_accuracy\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tylerfolkman/anaconda/envs/CarND-Traffic-Sign-Classifier-Project/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tylerfolkman/anaconda/envs/CarND-Traffic-Sign-Classifier-Project/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tylerfolkman/anaconda/envs/CarND-Traffic-Sign-Classifier-Project/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/tylerfolkman/anaconda/envs/CarND-Traffic-Sign-Classifier-Project/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tylerfolkman/anaconda/envs/CarND-Traffic-Sign-Classifier-Project/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_validation, y_validation)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, 'lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "Once you are completely satisfied with your model, evaluate the performance of the model on the test set.\n",
    "\n",
    "Be sure to only do this once!\n",
    "\n",
    "If you were to measure the performance of your trained model on the test set, then improve your model, and then measure the performance of your model on the test set again, that would invalidate your test results. You wouldn't get a true measure of how well your model would perform against real data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:CarND-Traffic-Sign-Classifier-Project]",
   "language": "python",
   "name": "conda-env-CarND-Traffic-Sign-Classifier-Project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
