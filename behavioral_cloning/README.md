
I started this project off with a simple strategy, use the [comma ai model](https://github.com/commaai/research/blob/master/train_steering_model.py) on the center images. To test that my pipeline was working, I made sure that I could overfit to three images - one straight, one left, and one right. I discovered that subtracting a mean image and dividing my a mean standard deviation worked much better than dividing the pixels by 255.

I then trained the model for 5-10 epochs on the center images, but when tested on the track, failed pretty miserably. I could only got for 5 to 10 seconds before driving left off the track. This is when I discovered that using a validation set wasn't too helpful as the validation MSE could by good, but the driving results poor. From this point forward, I used all the data for training and evaluated various epochs on the simulator.

My next strategy was to add the left and right camera angles and adjust the angle by 0.15. This helped a little, but still failed to get the car to drive more than about 20 seconds on the track. 

At this point, I started to read more on the confluence and slack channel about some strategies others have tried. That is how I came upon this [post](https://chatbotslife.com/learning-human-driving-behavior-using-nvidias-neural-network-model-and-image-augmentation-80399360efee#.lsuveu9f6) as well as the [NVIDIA paper](https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/). From reading these two sources, I decided to (1) try the NVIDIA model instead of the comma ai model and (2) perform significantly more data augmentation. From the blog post, I utilized his method of augmenting the brightness of the images. I also used the image translation and rotation augmentation mentioned in the NVIDIA paper. For rotation, I only flipped the images and flipped the steering angle. For translation, I used a mean zero, standard deviation 15 normal distribution and adjusted the steering angle by .006 for each change in x pixel. These steps were similar, but slightly modified from the blog post as well. From the blog post, I also used the image cropping as from my tests, it seemed to be effective at isolating the road section of the image. Lastly, I randomly chose a left, right, or center image. Adjusting the left and right images by 0.15 and -0.15 respectively. 

On the training side, I added dropout layers to the NVIDIA model after each dense layer and upsampled images with high steering angles to prevent the car from being biased towards only driving straight. This is mentioned in both the NVIDIA paper as well as the blog post. I also add validation data as it is being sampled from a much larger, augmented set of data.