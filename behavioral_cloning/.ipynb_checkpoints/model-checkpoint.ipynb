{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, ELU\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_dir = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def select_sample(data, n=1):\n",
    "    np.random.shuffle(data)\n",
    "    negatives = []\n",
    "    positives = []\n",
    "    straights = []\n",
    "    for d in data:\n",
    "        if d[1] > 0:\n",
    "            positives.append(d)\n",
    "        elif d[1] < 0:\n",
    "            negatives.append(d)\n",
    "        else:\n",
    "            straights.append(d)\n",
    "    return np.array(negatives[:n] + positives[:n] + straights[:n])\n",
    "\n",
    "def get_model():\n",
    "    ch, row, col = 3, 80, 160  # camera format\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(16, 8, 8, subsample=(4, 4), border_mode=\"same\", input_shape=(row, col, ch)))\n",
    "    model.add(ELU())\n",
    "    model.add(Convolution2D(32, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n",
    "    model.add(ELU())\n",
    "    model.add(Convolution2D(64, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "def preprocess(img, mean, std):\n",
    "    img = img.astype(\"float64\")\n",
    "    img -= mean\n",
    "    img /= (std + K.epsilon())\n",
    "    img = cv2.resize(img,None,fx=1/2, fy=1/2, interpolation = cv2.INTER_AREA)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "    \n",
    "def data_gen(data, mean, std, batch_size=32, shuffle=True, predict=False):\n",
    "    i = 0\n",
    "    n = data.shape[0]\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(data)\n",
    "        X, y = get_batch(data, i, batch_size, mean, std)\n",
    "        i = i+batch_size\n",
    "        if i > n:\n",
    "            i = 0\n",
    "        if predict:\n",
    "            yield X\n",
    "        else:\n",
    "            yield X, y\n",
    "        \n",
    "                \n",
    "def get_batch(data, start_index, batch_size, mean, std):\n",
    "    n = data.shape[0]\n",
    "    data_batch = data[start_index:min(start_index+batch_size, n)]\n",
    "    X = np.zeros((batch_size, 80, 160, 3))\n",
    "    y = np.zeros(batch_size)\n",
    "    for i, d in enumerate(data_batch):\n",
    "        img = plt.imread(base_dir + d[0].strip())\n",
    "        img = preprocess(img, mean, std)\n",
    "        X[i,:,:,:] = img\n",
    "        y[i] = d[1]\n",
    "    return X, y\n",
    "\n",
    "            \n",
    "def normalize(X):\n",
    "    ##keras code used on a sample of data to get approx of mean and std of image data\n",
    "    channel_index = 3\n",
    "    row_index = 1\n",
    "    col_index = 2\n",
    "    mean = np.mean(X, axis=(0, row_index, col_index))\n",
    "    broadcast_shape = [1, 1, 1]\n",
    "    broadcast_shape[channel_index - 1] = X.shape[channel_index]\n",
    "    mean = np.reshape(mean, broadcast_shape)\n",
    "    print(\"mean: {}\".format(mean))\n",
    "    X -= mean\n",
    "    \n",
    "    std = np.std(X, axis=(0, row_index, col_index))\n",
    "    broadcast_shape = [1, 1, 1]\n",
    "    broadcast_shape[channel_index - 1] = X.shape[channel_index]\n",
    "    std = np.reshape(std, broadcast_shape)\n",
    "    print(\"std: {}\".format(std))\n",
    "    X /= (std + K.epsilon())\n",
    "        \n",
    "def predict_gen(data, mean, std):\n",
    "    while 1:\n",
    "        for d in data:\n",
    "            img = plt.imread(base_dir + d[0])\n",
    "            img = preprocess(img, mean, std)\n",
    "            yield img\n",
    "            \n",
    "def read_data(array):\n",
    "    ## used for reading some test data\n",
    "    array_size = array.shape[0]\n",
    "    X_train = np.zeros((array_size, 160, 320, 3))\n",
    "    y_train = np.zeros(array_size)\n",
    "    print(X_train.shape)\n",
    "    for i, x in enumerate(array):\n",
    "        img = plt.imread(base_dir + x[0])\n",
    "        X_train[i,:,:,:] = img\n",
    "        y_train[i] = x[1]\n",
    "    return X_train, y_train \n",
    "\n",
    "\n",
    "def read_file(file_name, adjustment_value, flipped):\n",
    "    data = pd.read_csv(file_name)[['center', 'left', 'right', 'steering']]\n",
    "    center_data = data[['center', 'steering']].copy().values\n",
    "    left_data = data[['left', 'steering']].copy()\n",
    "    left_data.steering = left_data.steering + adjustment_value\n",
    "    left_data = left_data.values\n",
    "    right_data = data[['right', 'steering']].copy()\n",
    "    right_data.steering = right_data.steering - adjustment_value\n",
    "    right_data = right_data.values\n",
    "    all_data = np.concatenate([center_data, left_data, right_data])\n",
    "    if flipped:\n",
    "        for i in range(all_data.shape[0]):\n",
    "            all_data[i][0] = all_data[i][0].replace(\"IMG/\", \"IMG_FLIP/\")\n",
    "            value = all_data[i][1]\n",
    "            if value != 0:\n",
    "                value = value * -1\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adjustment_value = 0.2\n",
    "\n",
    "normal_data = read_file(\"./data/driving_log.csv\", adjustment_value, False)\n",
    "flipped_data = read_file(\"./data/driving_log.csv\", adjustment_value, True)\n",
    "data = np.concatenate([normal_data, flipped_data])\n",
    "#sample = select_sample(data, n=1024)\n",
    "#X, y = read_data(sample)\n",
    "#normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean = np.array([[[129.02680093, 133.27253171, 121.95475447]]], dtype='float64')\n",
    "std = np.array([[[47.02422432, 46.02757431, 57.58701242]]], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "48224/48216 [==============================] - 319s - loss: 0.0174 - mean_squared_error: 0.0174   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerfolkman/anaconda/envs/carnd/lib/python3.5/site-packages/keras/engine/training.py:1527: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x121ece160>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = data\n",
    "epochs = 1\n",
    "validation_data = data\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "model.fit_generator(data_gen(training_data, mean, std, 32), samples_per_epoch=training_data.shape[0],\n",
    "                    nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model.predict_generator(data_gen(training_data, mean, std, 3, False, True), training_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"model.h5\")\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as outfile:\n",
    "    outfile.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:carnd]",
   "language": "python",
   "name": "conda-env-carnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
