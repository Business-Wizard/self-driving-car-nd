{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, ELU\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_dir = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def select_sample(data, n=1):\n",
    "    np.random.shuffle(data)\n",
    "    negatives = []\n",
    "    positives = []\n",
    "    straights = []\n",
    "    for d in data:\n",
    "        if d[1] > 0:\n",
    "            positives.append(d)\n",
    "        elif d[1] < 0:\n",
    "            negatives.append(d)\n",
    "        else:\n",
    "            straights.append(d)\n",
    "    return np.array(negatives[:n] + positives[:n] + straights[:n])\n",
    "\n",
    "def get_model():\n",
    "    ch, row, col = 3, 80, 160  # camera format\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(16, 8, 8, subsample=(4, 4), border_mode=\"same\", input_shape=(row, col, ch)))\n",
    "    model.add(ELU())\n",
    "    model.add(Convolution2D(32, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n",
    "    model.add(ELU())\n",
    "    model.add(Convolution2D(64, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "def preprocess(img, mean, std):\n",
    "    img = img.astype(\"float64\")\n",
    "    img -= mean\n",
    "    img /= (std + K.epsilon())\n",
    "    img = cv2.resize(img,None,fx=1/2, fy=1/2, interpolation = cv2.INTER_AREA)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "    \n",
    "def data_gen(data, mean, std, batch_size=32, shuffle=True, predict=False):\n",
    "    i = 0\n",
    "    n = data.shape[0]\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(data)\n",
    "        X, y = get_batch(data, i, batch_size, mean, std)\n",
    "        i = i+batch_size\n",
    "        if i > n:\n",
    "            i = 0\n",
    "        if predict:\n",
    "            yield X\n",
    "        else:\n",
    "            yield X, y\n",
    "        \n",
    "                \n",
    "def get_batch(data, start_index, batch_size, mean, std):\n",
    "    n = data.shape[0]\n",
    "    data_batch = data[start_index:min(start_index+batch_size, n)]\n",
    "    X = np.zeros((batch_size, 80, 160, 3))\n",
    "    y = np.zeros(batch_size)\n",
    "    for i, d in enumerate(data_batch):\n",
    "        img = plt.imread(base_dir + d[0])\n",
    "        img = preprocess(img, mean, std)\n",
    "        X[i,:,:,:] = img\n",
    "        y[i] = d[1]\n",
    "    return X, y\n",
    "\n",
    "            \n",
    "def normalize(X):\n",
    "    ##keras code used on a sample of data to get approx of mean and std of image data\n",
    "    channel_index = 3\n",
    "    row_index = 1\n",
    "    col_index = 2\n",
    "    mean = np.mean(X, axis=(0, row_index, col_index))\n",
    "    broadcast_shape = [1, 1, 1]\n",
    "    broadcast_shape[channel_index - 1] = X.shape[channel_index]\n",
    "    mean = np.reshape(mean, broadcast_shape)\n",
    "    print(\"mean: {}\".format(mean))\n",
    "    X -= mean\n",
    "    \n",
    "    std = np.std(X, axis=(0, row_index, col_index))\n",
    "    broadcast_shape = [1, 1, 1]\n",
    "    broadcast_shape[channel_index - 1] = X.shape[channel_index]\n",
    "    std = np.reshape(std, broadcast_shape)\n",
    "    print(\"std: {}\".format(std))\n",
    "    X /= (std + K.epsilon())\n",
    "        \n",
    "def predict_gen(data, mean, std):\n",
    "    while 1:\n",
    "        for d in data:\n",
    "            img = plt.imread(base_dir + d[0])\n",
    "            img = preprocess(img, mean, std)\n",
    "            yield img\n",
    "            \n",
    "def read_data(array):\n",
    "    ## used for reading some test data\n",
    "    array_size = array.shape[0]\n",
    "    X_train = np.zeros((array_size, 160, 320, 3))\n",
    "    y_train = np.zeros(array_size)\n",
    "    print(X_train.shape)\n",
    "    for i, x in enumerate(array):\n",
    "        img = plt.imread(base_dir + x[0])\n",
    "        X_train[i,:,:,:] = img\n",
    "        y_train[i] = x[1]\n",
    "    return X_train, y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/driving_log.csv\")[['center', 'steering']].values\n",
    "sample = select_sample(data, n=512)\n",
    "#X, y = read_data(sample)\n",
    "#normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean = np.array([[[129.02680093, 133.27253171, 121.95475447]]], dtype='float64')\n",
    "std = np.array([[[47.02422432, 46.02757431, 57.58701242]]], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1536/1536 [==============================] - 8s - loss: 0.0580 - mean_squared_error: 0.0580     \n",
      "Epoch 2/50\n",
      "1536/1536 [==============================] - 7s - loss: 0.0187 - mean_squared_error: 0.0187     \n",
      "Epoch 3/50\n",
      "1536/1536 [==============================] - 7s - loss: 0.0104 - mean_squared_error: 0.0104     \n",
      "Epoch 4/50\n",
      "1536/1536 [==============================] - 7s - loss: 0.0088 - mean_squared_error: 0.0088     \n",
      "Epoch 5/50\n",
      "1536/1536 [==============================] - 7s - loss: 0.0068 - mean_squared_error: 0.0068     \n",
      "Epoch 6/50\n",
      "1536/1536 [==============================] - 7s - loss: 0.0064 - mean_squared_error: 0.0064     \n",
      "Epoch 7/50\n",
      "1024/1536 [===================>..........] - ETA: 2s - loss: 0.0064 - mean_squared_error: 0.0064"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cb3a418f72d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m model.fit_generator(data_gen(training_data, mean, std, 32), samples_per_epoch=training_data.shape[0],\n\u001b[0;32m----> 8\u001b[0;31m                     nb_epoch=epochs)\n\u001b[0m",
      "\u001b[0;32m/Users/tylerfolkman/anaconda/envs/carnd/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, **kwargs)\u001b[0m\n\u001b[1;32m    922\u001b[0m                                         \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m                                         pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     def evaluate_generator(self, generator, val_samples,\n",
      "\u001b[0;32m/Users/tylerfolkman/anaconda/envs/carnd/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1472\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m                         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_data = sample\n",
    "epochs = 50\n",
    "validation_data = sample\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "model.fit_generator(data_gen(training_data, mean, std, 32), samples_per_epoch=training_data.shape[0],\n",
    "                    nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.predict_generator(data_gen(training_data, mean, std, 3, False, True), training_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:carnd]",
   "language": "python",
   "name": "conda-env-carnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
